"""
immich-ml-metal: Metal/ANE-optimized ML service for Immich.

Drop-in replacement for Immich's ML service, optimized for Apple Silicon.
"""

from fastapi import FastAPI, Form, File, UploadFile, HTTPException, Request
from fastapi.responses import ORJSONResponse, PlainTextResponse, JSONResponse
from typing import Optional
import json
import numpy as np
from PIL import Image
import io
import os
import logging
import asyncio
from contextlib import asynccontextmanager
from pydantic import BaseModel, Field

from .config import settings

# Configure logging based on settings
settings.configure_logging()
logger = logging.getLogger(__name__)

# Use real models unless STUB_MODE is set
STUB_MODE = os.getenv("STUB_MODE", "false").lower() == "true"

if STUB_MODE:
    logger.warning("Running in STUB_MODE - returning fake data")

# Semaphore for backpressure - limits queued requests
_request_semaphore: Optional[asyncio.Semaphore] = None


def get_request_semaphore() -> asyncio.Semaphore:
    """Get or create the request semaphore (lazy init for async context)."""
    global _request_semaphore
    if _request_semaphore is None:
        _request_semaphore = asyncio.Semaphore(settings.max_concurrent_requests)
    return _request_semaphore


# Pydantic models for response validation
class BoundingBox(BaseModel):
    x1: int
    y1: int
    x2: int
    y2: int


class FaceDetection(BaseModel):
    boundingBox: BoundingBox
    embedding: str  # Stringified array
    score: float


class OCRResult(BaseModel):
    text: list[str]
    box: list[int]  # Flat list of coordinates
    boxScore: list[float]
    textScore: list[float]


class PredictResponse(BaseModel):
    imageHeight: Optional[int] = None
    imageWidth: Optional[int] = None
    clip: Optional[str] = None  # Stringified array
    facial_recognition: Optional[list[FaceDetection]] = Field(None, alias="facial-recognition")
    ocr: Optional[OCRResult] = None


# Lifespan context for startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handle startup and shutdown events."""
    logger.info("Starting immich-ml-metal service")
    logger.info(f"STUB_MODE: {STUB_MODE}")
    logger.info(f"Settings: host={settings.host}, port={settings.port}")
    logger.info(f"CLIP model: {settings.clip_model}")
    logger.info(f"Face model: {settings.face_model}")
    logger.info(f"Face min score: {settings.face_min_score}")
    logger.info(f"Max concurrent requests: {settings.max_concurrent_requests}")
    logger.info(f"Log level: {settings.log_level}")
    
    yield
    
    # Cleanup on shutdown - import modules to access current state
    logger.info("Shutting down immich-ml-metal service")
    if not STUB_MODE:
        try:
            from .models import clip as clip_module
            from .models import face_embed as face_module
            
            # Cleanup CLIP model
            with clip_module._model_lock:
                if clip_module._current_model is not None:
                    clip_module._current_model.unload()
                    clip_module._current_model = None
                    clip_module._current_model_name = None
            
            # Cleanup face recognition model
            face_module.unload_recognition_model()
            
            logger.info("Models unloaded successfully")
        except Exception as e:
            logger.error(f"Error during model cleanup: {e}")


app = FastAPI(
    title="immich-ml-metal",
    description="Metal/ANE-optimized drop-in replacement for Immich ML",
    version="0.1.0",
    lifespan=lifespan
)


# Middleware for request logging (conditional based on settings)
@app.middleware("http")
async def log_requests(request: Request, call_next):
    if settings.log_requests:
        logger.info(f"{request.method} {request.url.path}")
    response = await call_next(request)
    if settings.log_requests:
        logger.debug(f"Response status: {response.status_code}")
    return response


def get_clip(model_name: str = "ViT-B-32__openai"):
    """Get CLIP model, loading on first use or switching if model changed."""
    if STUB_MODE:
        return None
    from .models.clip import get_clip_model
    return get_clip_model(model_name)


async def run_face_recognition_async(
    image_bytes: bytes, 
    min_score: float, 
    model_name: str
) -> list[dict]:
    """Run face detection and embedding generation (async wrapper)."""
    return await asyncio.to_thread(
        _run_face_recognition_sync,
        image_bytes,
        min_score,
        model_name
    )


def _run_face_recognition_sync(
    image_bytes: bytes,
    min_score: float,
    model_name: str
) -> list[dict]:
    """Synchronous face recognition implementation."""
    from .models.face_detect import detect_faces
    from .models.face_embed import get_face_embedding, get_face_embedding_from_bbox
    
    faces, _, _ = detect_faces(image_bytes)
    
    results = []
    for face in faces:
        if face["score"] < min_score:
            continue
        
        if "landmarks" in face:
            embedding = get_face_embedding(image_bytes, face["landmarks"], model_name)
        else:
            embedding = get_face_embedding_from_bbox(image_bytes, face["boundingBox"], model_name)
        
        if embedding is not None:
            results.append({
                "boundingBox": face["boundingBox"],
                "embedding": str(embedding.tolist()),
                "score": face["score"]
            })
    
    return results


@app.get("/")
async def root():
    """Root endpoint - mirrors Immich ML."""
    return ORJSONResponse({"message": "Immich ML"})


@app.get("/ping")
def ping():
    """Health check endpoint."""
    return PlainTextResponse("pong")


@app.get("/health")
async def health():
    """
    Detailed health check endpoint.
    
    Checks if models can be loaded and basic functionality works.
    """
    health_status = {
        "status": "healthy",
        "stub_mode": STUB_MODE,
        "checks": {}
    }
    
    try:
        if not STUB_MODE:
            # Check CLIP model
            try:
                clip = get_clip(settings.clip_model)
                health_status["checks"]["clip"] = "ok"
            except Exception as e:
                logger.error(f"CLIP health check failed: {e}")
                health_status["checks"]["clip"] = f"error: {str(e)}"
                health_status["status"] = "degraded"
            
            # Check face recognition model
            try:
                from .models.face_embed import get_recognition_model
                get_recognition_model(settings.face_model)
                health_status["checks"]["face_recognition"] = "ok"
            except Exception as e:
                logger.error(f"Face recognition health check failed: {e}")
                health_status["checks"]["face_recognition"] = f"error: {str(e)}"
                health_status["status"] = "degraded"
            
            # Actually test Vision framework with a minimal image
            try:
                from .models.face_detect import detect_faces
                # Create 1x1 test image
                test_img = Image.new("RGB", (1, 1), color=(128, 128, 128))
                buffer = io.BytesIO()
                test_img.save(buffer, format="JPEG")
                detect_faces(buffer.getvalue())
                health_status["checks"]["vision_framework"] = "ok"
            except Exception as e:
                logger.error(f"Vision framework health check failed: {e}")
                health_status["checks"]["vision_framework"] = f"error: {str(e)}"
                health_status["status"] = "degraded"
        else:
            health_status["checks"]["stub_mode"] = "active"
        
        return JSONResponse(content=health_status)
        
    except Exception as e:
        logger.error(f"Health check failed: {e}", exc_info=True)
        return JSONResponse(
            content={"status": "unhealthy", "error": str(e)},
            status_code=503
        )


@app.post("/predict")
async def predict(
    entries: str = Form(...),
    image: Optional[UploadFile] = File(None),
    text: Optional[str] = Form(None),
):
    """
    Main prediction endpoint - mirrors Immich ML API.
    
    Args:
        entries: JSON string describing requested tasks
        image: Optional image file for visual tasks
        text: Optional text for text encoding tasks
    
    Returns:
        ORJSONResponse with inference results
    """
    # Apply backpressure via semaphore
    semaphore = get_request_semaphore()
    try:
        # Use timeout to avoid indefinite queuing
        async with asyncio.timeout(settings.request_timeout):
            async with semaphore:
                return await _process_predict(entries, image, text)
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=503,
            detail="Service overloaded, request timed out waiting in queue"
        )


async def _process_predict(
    entries: str,
    image: Optional[UploadFile],
    text: Optional[str],
) -> ORJSONResponse:
    """Internal predict processing (assumes semaphore is held)."""
    # Parse the entries JSON
    try:
        tasks = json.loads(entries)
    except json.JSONDecodeError as e:
        logger.error(f"Invalid entries JSON: {e}")
        raise HTTPException(status_code=422, detail=f"Invalid entries JSON: {e}")
    
    if image is None and text is None:
        raise HTTPException(
            status_code=400, 
            detail="Either image or text must be provided"
        )
    
    response = {}
    
    # Read and validate image if provided
    image_bytes = None
    img = None
    if image:
        if image.size and image.size > settings.max_image_size:
            raise HTTPException(
                status_code=413,
                detail=f"Image too large. Max size: {settings.max_image_size / 1024 / 1024:.1f}MB"
            )
        
        try:
            image_bytes = await image.read()
            img = Image.open(io.BytesIO(image_bytes))
            response["imageHeight"] = img.height
            response["imageWidth"] = img.width
            
            if img.width * img.height > 100_000_000:  # 100 megapixels
                raise HTTPException(
                    status_code=413,
                    detail="Image resolution too high"
                )
                
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to read/decode image: {e}")
            raise HTTPException(status_code=400, detail=f"Invalid image: {e}")
    
    # Process each requested task
    for task_type, task_config in tasks.items():
        
        if task_type == "clip":
            if "visual" in task_config and image_bytes:
                model_name = task_config["visual"].get("modelName", settings.clip_model)
                
                if STUB_MODE:
                    embedding = np.random.randn(512).astype(np.float32)
                    embedding = embedding / np.linalg.norm(embedding)
                else:
                    clip = get_clip(model_name)
                    embedding = await asyncio.to_thread(
                        clip.encode_image,
                        image_bytes
                    )
                
                response["clip"] = str(embedding.tolist())
                
            elif "textual" in task_config and text:
                model_name = task_config["textual"].get("modelName", settings.clip_model)
                
                if STUB_MODE:
                    embedding = np.random.randn(512).astype(np.float32)
                    embedding = embedding / np.linalg.norm(embedding)
                else:
                    clip = get_clip(model_name)
                    embedding = await asyncio.to_thread(
                        clip.encode_text,
                        text
                    )
                
                response["clip"] = str(embedding.tolist())
        
        elif task_type == "facial-recognition":
            if image_bytes is None:
                continue
                
            detection_config = task_config.get("detection", {})
            recognition_config = task_config.get("recognition", {})
            
            min_score = detection_config.get("options", {}).get(
                "minScore", 
                settings.face_min_score
            )
            model_name = recognition_config.get("modelName", settings.face_model)
            
            if STUB_MODE:
                fake_embedding = np.random.randn(512).astype(np.float32).tolist()
                faces = [
                    {
                        "boundingBox": {
                            "x1": int(img.width * 0.25),
                            "y1": int(img.height * 0.15),
                            "x2": int(img.width * 0.75),
                            "y2": int(img.height * 0.85)
                        },
                        "embedding": str(fake_embedding),
                        "score": 0.99
                    }
                ]
            else:
                faces = await run_face_recognition_async(
                    image_bytes,
                    min_score,
                    model_name
                )
            
            response["facial-recognition"] = faces
        
        elif task_type == "ocr":
            if image_bytes is None:
                continue
                
            detection_config = task_config.get("detection", {})
            recognition_config = task_config.get("recognition", {})
            
            min_detection_score = detection_config.get("options", {}).get("minScore", 0.0)
            min_recognition_score = recognition_config.get("options", {}).get("minScore", 0.0)
            min_score = max(min_detection_score, min_recognition_score)
            
            if STUB_MODE:
                response["ocr"] = {
                    "text": ["placeholder", "text"],
                    "box": [0, 0, 100, 0, 100, 50, 0, 50, 0, 50, 100, 50, 100, 100, 0, 100],
                    "boxScore": [0.95, 0.92],
                    "textScore": [0.98, 0.96]
                }
            else:
                from .models.ocr import recognize_text
                ocr_result = await asyncio.to_thread(
                    recognize_text,
                    image_bytes,
                    min_confidence=min_score,
                    use_language_correction=settings.ocr_use_language_correction
                )
                response["ocr"] = ocr_result
    
    # Validate response against schema - fail loudly if validation fails
    try:
        validated_response = PredictResponse(**response)
        return ORJSONResponse(validated_response.model_dump(by_alias=True, exclude_none=True))
    except Exception as e:
        logger.error(f"Response validation failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Internal error: response validation failed"
        )


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler for unexpected errors."""
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    
    # Only expose error details in debug mode (should be off for network-exposed service)
    error_detail = str(exc) if settings.debug_mode else "Internal server error"
    
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "error": error_detail}
    )


def main():
    """Entry point for running the service directly."""
    import uvicorn
    
    logger.info(f"Starting server on {settings.host}:{settings.port}")
    
    uvicorn.run(
        "src.main:app",
        host=settings.host,
        port=settings.port,
        reload=False,
        log_level=settings.log_level.lower(),
        timeout_keep_alive=settings.request_timeout,
    )


if __name__ == "__main__":
    main()