# Core web framework
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
orjson>=3.9.0
python-multipart>=0.0.6

# Image processing
pillow>=10.0.0
numpy>=1.26.0,<2.0
opencv-python>=4.9.0

# Apple frameworks via PyObjC
pyobjc-framework-Vision>=10.0
pyobjc-framework-Quartz>=10.0
pyobjc-framework-CoreML>=10.0

# MLX for CLIP (Apple Silicon optimized)
mlx>=0.4.0
huggingface-hub>=0.20.0

# Pin mlx-clip to specific commit for stability
# To find current commit: git ls-remote https://github.com/harperreed/mlx_clip.git HEAD
# TODO: Replace COMMIT_HASH with actual commit hash before deploying
mlx-clip @ git+https://github.com/harperreed/mlx_clip.git@f56e3ecc72c74c68b6b50eb6f50c3f22fc23fe2c

# Fallback if you want to use open_clip instead of mlx-clip:
# open-clip-torch>=2.24.0

# Face recognition - official onnxruntime now supports Apple Silicon via CoreML
# No need for onnxruntime-silicon anymore as of v1.17+
onnxruntime>=1.17.0
insightface>=0.7.3

# Development/testing
pytest>=8.0.0
pytest-asyncio>=0.23.0
httpx>=0.26.0